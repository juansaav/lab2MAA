{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de aprendizaje automático 2017 laboratorio 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### En esta primera etapa se realizará un experimento para mostrar cómo se puede aplicar erroneamente la selección de características."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relizamos la importación de los módulos necesarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "from sklearn import feature_selection\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos una matriz con 10000 características aleatorias y una etiqueta a predecir también con valores aleatorios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "X = np.random.randint(0,10,size=[100,10000])\n",
    "y = np.random.randint(0,2,size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos feature selection con el método de chi cuadrado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fs = feature_selection.SelectKBest(feature_selection.chi2, k=20)\n",
    "X_fs = fs.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generamos un clasificador. En este caso una regresión logística."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos validación cruzada para ver los resultados obtenidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.95238095  0.85714286  0.8         0.89473684  0.78947368]\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(clf, X_fs, y, cv=5, scoring='accuracy')\n",
    "print scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explicar por qué si los números son generados al azar se obtienen buenos resultados. ¿Cómo solucionaría este problema?\n",
    "Resolver el problema utilizando pipelines de sklearn\n",
    "\n",
    "Respuesta : \n",
    " Si se observa la siguiente línea: feature_selection.SelectKBest(feature_selection.chi2, k=20) . \n",
    " Lo que hace esta línea es tomar los 20 atributos más representativos de X segun las target, utilizando el test\n",
    " de chi2, que calcula las dimensiones de X más independientes del target Y y las elimina. \n",
    " Esta selección de atributos utilizando chi2 es apropiada para los casos de aprendizaje supervisado en problemas de regresión,\n",
    " es decir aquellos que tienen uno o mas valores en target que toman valores continuos, por ejemplo medir el largo\n",
    " de un animal a partir de su edad y peso. Los problemas donde los valores en target son discretos se llaman problemas\n",
    " de clasificación, este es nuestro caso ya que Y toma valores en 1..10. Para descartar que el problema venga por ese lado\n",
    " se hizo una prueba utilizando f_classif para select_k_best en vez de chi2.\n",
    " \n",
    " Prueba 1\n",
    " \n",
    " Resultado [ 0.95238095  0.9         0.9         0.9         0.94736842]\n",
    "\n",
    " Como vemos el resultado sigue siendo bueno (valores cercanos a 1).\n",
    " El problema entonces se encuentra en la seleccion de los k mejores atributos , se observa que utilizamos todo el conjunto X       para esta selección, luego en la validación, y por lo tanto los resultados son mejores que si se valida contra un conjunto de  ejemplos que no se haya utilizado en el entrenamiento ni en la selección de atributos, como se muestra en la siguiente prueba:\n",
    " \n",
    " Prueba 2\n",
    " \n",
    " Creamos el siguiente pipeline:\n",
    " Pipeline([('fs2', feature_selection.SelectKBest(feature_selection.chi2,k=20)), ('clf2',LogisticRegression()),]) \n",
    " luego aplicamos validación cruzada utilizando este pipeline sobre el conjunto X.\n",
    " De esta forma para cada iteración se aplica primero select_k_best en base al conjunto entrenamiento y luego se valida\n",
    " contra el conjunto test, el cual se utiliza únicamente en esta instancia.\n",
    " \n",
    " Resultado [ 0.47619048  0.55        0.35        0.5         0.63157895]\n",
    " \n",
    " Observamos resultados maá razonables considerando que los datos son aleatorios. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.80952381  0.85714286  1.          0.89473684  0.78947368]\n",
      "[ 0.28571429  0.52380952  0.5         0.63157895  0.47368421]\n"
     ]
    }
   ],
   "source": [
    "#Prueba 1\n",
    "fs1 = feature_selection.SelectKBest(feature_selection.f_classif, k=20)\n",
    "X_fs1 = fs1.fit_transform(X, y)\n",
    "scores1 = cross_val_score(clf, X_fs1, y, cv=5, scoring='accuracy')\n",
    "print scores1\n",
    "\n",
    "#Prueba 2, resolucion ejercicio\n",
    "from sklearn.pipeline import Pipeline\n",
    "a2 = Pipeline([('fs2', feature_selection.SelectKBest(feature_selection.chi2,k=20)),('clf2',LogisticRegression()),])\n",
    "a2 = a2.fit(X, y)\n",
    "scores = cross_val_score(a2, X, y, cv=5, scoring='accuracy')\n",
    "print scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para la siguiente parte trabajaremos con un conjunto de datos de textos. Para ello los descargamos.\n",
    "Una descripción del set de datos: http://qwone.com/~jason/20Newsgroups/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "train = fetch_20newsgroups(subset='train', categories=['talk.politics.guns', 'soc.religion.christian', 'comp.graphics', 'sci.med'], shuffle=True, random_state=42)\n",
    "test = fetch_20newsgroups(subset='test', categories=['talk.politics.guns', 'soc.religion.christian', 'comp.graphics', 'sci.med'], shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mostrar un ejemplo de entrenamiento de cada una de las categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comp.graphics\n",
      "Subject: newss\n",
      "From: pollarda@physc1.byu.edu\n",
      "Distribution: world\n",
      "Organization: Brigham Young University\n",
      "Lines: 24\n",
      "\n",
      "\n",
      "\n",
      "I am working on a project where we are going to be including\n",
      "both still and moving grapics within a database.  Of course\n",
      "JPEG and MPEG come to mind as the formats of choice for the\n",
      "various files.  However, from what I read on the Net, it seems\n",
      "as if there are several different forms of each of these.\n",
      "\n",
      "What I want to do, is settle on a file format which I can count\n",
      "on as being a standard format 10 years from now.  I know Apple is going\n",
      "to support Quicktime on the new Power PC's and, so this\n",
      "may be the format of choice.\n",
      "\n",
      "What format does Apple's Quicktime use for their products?  I guess\n",
      "it is some kind of MPEG for their motion picture. Is it any different\n",
      "than standard MPEG files?\n",
      "\n",
      "Thanx for any info!\n",
      "\n",
      "Art.\n",
      "Pollarda@xray.byu.edu\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "sci.med\n",
      "From: annick@cortex.physiol.su.oz.au (Annick Ansselin)\n",
      "Subject: Re: Is MSG sensitivity superstition?\n",
      "Nntp-Posting-Host: cortex.physiol.su.oz.au\n",
      "Organization: Department of Physiology, University of Sydney, NSW, Australia\n",
      "Lines: 29\n",
      "\n",
      "In <C5nFDG.8En@sdf.lonestar.org> marco@sdf.lonestar.org (Steve Giammarco) writes:\n",
      "\n",
      ">>\n",
      ">>And to add further fuel to the flame war, I read about 20 years ago that\n",
      ">>the \"natural\" MSG - extracted from the sources you mention above - does not\n",
      ">>cause the reported aftereffects; it's only that nasty \"artificial\" MSG -\n",
      ">>extracted from coal tar or whatever - that causes Chinese Restaurant\n",
      ">>Syndrome.  I find this pretty hard to believe; has anyone else heard it?\n",
      "\n",
      "MSG is mono sodium glutamate, a fairly straight forward compound. If it is\n",
      "pure, the source should not be a problem. Your comment suggests that \n",
      "impurities may be the cause.\n",
      "My experience of MSG effects (as part of a double blind study) was that the\n",
      "pure stuff caused me some rather severe effects.\n",
      "\n",
      ">I was under the (possibly incorrect) assumption that most of the MSG on\n",
      ">our foods was made from processing sugar beets. Is this not true? Are \n",
      ">there other sources of MSG?\n",
      "\n",
      "Soya bean, fermented cheeses, mushrooms all contain MSG. \n",
      "\n",
      ">I am one of those folx who react, sometimes strongly, to MSG. However,\n",
      ">I also react strongly to sodium chloride (table salt) in excess. Each\n",
      ">causes different symptoms except for the common one of rapid heartbeat\n",
      ">and an uncomfortable feeling of pressure in my chest, upper left quadrant.\n",
      "\n",
      "The symptoms I had were numbness of jaw muscles in the first instance\n",
      "followed by the arms then the legs, headache, lethargy and unable to keep\n",
      "awake. I think it may well affect people differently.\n",
      "\n",
      "soc.religion.christian\n",
      "From: Petch@gvg47.gvg.tek.com (Chuck Petch)\n",
      "Subject: Daily Verse\n",
      "Organization: Grass Valley Group, Grass Valley, CA\n",
      "Lines: 4\n",
      "\n",
      "For whoever does the will of my Father in heaven is my brother and sister\n",
      "and mother.\" \n",
      "\n",
      "Matthew 12:50\n",
      "\n",
      "talk.politics.guns\n",
      "From: tms@cs.umd.edu (Tom Swiss (not Swift, not Suiss, Swiss!))\n",
      "Subject: Re: Clinton wants National ID card, aka USSR-style \"Internal Passport\"\n",
      "Organization: The Reality Liberation Front (pixels to the people!)\n",
      "Lines: 17\n",
      "\n",
      "slp9k@cc.usu.edu writes:\n",
      ">\n",
      ">\tI just want to point out that while I am fully in support of privacy,\n",
      ">it will be possible soon to have a completely secure ID card, useable in bank\n",
      ">transactions, medical, etc etc.\n",
      "\n",
      "     There is no such thing as \"completely secure,\" especially when dealing\n",
      "with High Technology. It's all a question of cost: what cost are you\n",
      "willing to bear to protect your information vs. what rewards the \"bad guys\"\n",
      "are going to get if they break it. The rewards of breaking such a single ID\n",
      "system would be high indeed.\n",
      "\n",
      "===============================================================================\n",
      "Tom Swiss/tms@cs.umd.edu  |  \"Born to die\"   |   Keep your laws off my brain!\n",
      "     \"What's so funny 'bout peace, love and understanding?\" - Nick Lowe \n",
      "     This .sig contains no animal products and was not tested on animals.\n",
      "  \"Time is just nature's way of keeping everything from happening at once.\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for t in set(train.target):\n",
    "    listcat = [index for index,value in enumerate(train.target) if value == t][:1]\n",
    "    print(train.target_names[t])\n",
    "    print train.data[listcat[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revisar el módulo CountVectorizer para extraer caracterísitcas a partir de texto y explicar cómo funciona. Explicar qué es el modelo de bolsa de palabras.\n",
    "\n",
    "Respuesta : \n",
    "\n",
    " CountVectorizer es una librería para procesar textos y poder clasificarlos. Convierte una colección de documentos en una matriz de conteo de ocurrencias de palabras.\n",
    " Supongamos que se tienen N documentos cada uno de ellos pertenecientes a una categoria t (target) que se quiere predecir.\n",
    " CountVectorizer crea una matriz de tamaño N x cantF, donde cantF es el conjunto de palabras que aparecen en algún \n",
    " documento, estas son las features consideradas para el entrenamiento.\n",
    " Cada documento se considera una bolsa de palabras, es decir un conjunto de palabras sin importar su orden ni dependencia en el documento, y CV cuenta la cantidad de ocurrencias de cada palabra j para cada documento i y setea el valor  (i,j)=cantOcurrencias en la matriz. \n",
    " Al proceso de asignar un numero en el rango de [1..cantF] para cada palabra se le llama tokenización, y al de contar las  ocurrencias se le llama conteo.\n",
    " De esta forma, aplicando fit_transform ya tenemos un conjunto de entrenamiento dado por la matriz creada y podemos utilizarlo \n",
    " para entrenar, sin embargo hay un par de ajustes que pueden mejorar la performance. Uno de ellos es la cantidad de valores 0 \n",
    " que tiene nuestra matriz, supongamos que tenemos 100.000 features(conjunto de palabras en todos los documentos) y un documento \n",
    " pequeño con 200 palabras. Entonces en la row de ese documento vamos a tener como mínimo 99.800 valores en cero. La librería \n",
    " scipy.sparse soluciona este problema almacenando únicamente valores distintos de cero y ahorrando así mucha memoria.\n",
    " \n",
    " Otro de los procesos que se puede aplicar es el de normalización (explciado en la parte 10 del ejercicio). En resumen, \"bolsa de palabras\" es el proceso que sigue los pasos tokenización, conteo y normalizacion. Considerando únicamente ocurrencias de palabras e ignorando la posición relativa de las mismas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(train.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenar un clasificador bayesiano sencillo Multinomial. Explicar cada uno de los parámetros de este clasificador.\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "MultinomialNB es una implementación  del algoritmo naive Bayes para datos con distribución multinomial. La distribución es parametrizada por el vector ![](img/1.png)  para cada clase de Y, donde n es la cantidad de features y 0_yi es la probabilidad ![](img/2.png)  de que aparezca el feature  en una muestra perteneciente a la clase .\n",
    "Cada parámetro 0_y se calcula con una versión suavizada  de conteo de frecuencia relativa:![](img/3.png)\n",
    "con ![](img/4.png)  la cantidad de veces que aparece el feature i en la muestra de la clase y en el conjunto de entrenamiento T, y ![](img/5.png) es la cuenta total para todos los features en la muestra de la clase .\n",
    "\n",
    "El parámetro ∝ sirve para el suavizado de la función  y sirve para que la probabilidad de los features que no salieron en la muestra sea distinta de 0, si se usa ∝ =0 la función no tiene suavizado. Si ∝ =1 se le llama suavizado de Laplace, y si ∝ >1 se le llama suavizado de Lidstone.\n",
    "Los parámetros fit_prior y class_prior sirven para definir las probabilidades preferentes de las clases. Class_prior puede ser un arreglo del tamaño del conjunto de clases Y, o puede ser None. Si se pasa un arreglo, éste lleva los valores de probabilidades de cada clase. En caso de que el valor sea None las probabilidades de las clases se calculan en función de fit_prior. Si fit_prior es True se calcula las probabilidades en función de los datos, si es False se usa una probabilidad uniforme para todas las clases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True).fit(X_train_counts, train.target)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repetir el proceso pero utilizando el módulo TfidfTransformer en lugar de CountVectorizer. Explicar la medida Tfidf.\n",
    "\n",
    "Respuesta:\n",
    "\n",
    " Como se explicó en la parte de CountVectorizer, la matriz creada tiene algunos inconvenientes que pueden\n",
    " afectar la performance del clasificador. Supongamos un documento muy grande que pertenece a la misma categoría\n",
    " que un documento pequeño, la cantidad de ocurrencias de una palabra en común para el documento grande es mayor \n",
    " a la del documento chico, cuando en realidad estan representando lo mismo pero a distinta escala. Para solucionar esto simplemente se divide la cantidad de ocurrencias de cada palabra en un documento\n",
    " por la cantidad de palabras en el mismo, a estas nuevas features se les llama \"Term Frequencies\".\n",
    "\n",
    " Otra mejora esta en considerar menos relevantes palabras que aparecen repetidas en el cuerpo de varios documentos\n",
    " que aquellas que aparecen en una porción pequeña del cuerpo del documento, se ajustan los pesos relativos a cada palabra \n",
    " para solucionarlo. A esta técnica se le llama \"Term Frequency times Inverse Document Frequency\", y la combinación de ambas\n",
    " se puede aplicar utilizando TfidfTransformer, como se muestra en la siguiente prueba.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "clfb = MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True).fit(X_train_tfidf, train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluya los pasos de extracción de características y el clasificador dentro de un pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('count_vect', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "    ...False,\n",
       "         use_idf=True)), ('clf', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('count_vect', CountVectorizer()),('tfidf_transformer',TfidfTransformer()),('clf',MultinomialNB()),])\n",
    "pipeline.fit(train.data,train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluir las métricas de precision, recall y fscore para cada cada una de las clases y la matriz de confusión probando contra el conjunto de testing.\n",
    "\n",
    "Aclación: Se agregan las medidas solicitadas, pero se utiliza el parametro \"average=macro\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.96368715,  0.98765432,  0.80284553,  0.9383378 ]), array([ 0.88688946,  0.80808081,  0.99246231,  0.96153846]), array([ 0.92369478,  0.88888889,  0.88764045,  0.94979647]), array([389, 396, 398, 364]))\n",
      "[[345   2  29  13]\n",
      " [  9 320  57  10]\n",
      " [  2   1 395   0]\n",
      " [  2   1  11 350]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "predicted = pipeline.predict(test.data)\n",
    "res =  precision_recall_fscore_support(test.target, predicted, average='macro')\n",
    "print \"Precision: %f \" % res[0],\n",
    "print \" Recall: %f \" % res[1],\n",
    "print \" F-score: %f \" % res[2]\n",
    "\n",
    "print \"Confusion matrix\"\n",
    "print confusion_matrix(test.target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incluya una GridSearch para ver cuales son los mejores hiper-parámetros del modelo.\n",
    "\n",
    "Aclación: No se agregan todos los parámetros posibles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejores hiper-parametros encontraodos\n",
      "\tclf__alpha: 0.01\n",
      "\tcount_vect__max_df: 0.5\n",
      "\tcount_vect__max_features: 20000\n",
      "\tcount_vect__min_df: 1\n",
      "\ttfidf_transformer__norm: l2\n",
      "\ttfidf_transformer__sublinear_tf: True\n",
      "\ttfidf_transformer__use_idf: False\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "tuned_parameters = {  \n",
    "   'count_vect__max_df': (0.5, 1.0),  \n",
    "   'count_vect__max_features': (10000, 20000),  \n",
    "   'count_vect__min_df': (1, 5),  \n",
    "   'tfidf_transformer__use_idf': (True, False),  \n",
    "   'tfidf_transformer__sublinear_tf': (True, False),  \n",
    "   'tfidf_transformer__norm': ('l1', 'l2'),  \n",
    "   'clf__alpha': (0.1, 0.01)  \n",
    "   }\n",
    "grid_search = GridSearchCV(pipeline, tuned_parameters, cv=3,scoring='accuracy',n_jobs = 2)\n",
    "grid_search.fit(train.data, train.target)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "print 'Mejores hiper-parametros encontraodos'\n",
    "for param_name in sorted(list(tuned_parameters.keys())): print(\"\\t{0}: {1}\".format(param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explique cada uno de los parámetros de cada uno de los pasos del pipeline y pruebe con varios tipos de clasificadores de los vistos en el curso. ¿Con qué configuración de parámetros obtuvo mejores resultados? Aplique selección de atributos y valore los resultados obtenidos. Realice una prueba final contra el conjunto de pruebas.\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "Los parámetros que se modificaron en la grid search para cada paso del pipelines son: max_df,mi_df y max_features para el CountVectorizer, use_idf, sublinear_tf y norm para TfidfTranformer y alpha para multinomialNB.\n",
    "\n",
    "max_df : float in range [0.0, 1.0] or int, default=1.0\n",
    "Al armar el vocabulario de palabras identificadas por countVectorizer, max_df es la frecuencia máxima que puede tener una palabra, no se tienen en cuenta las palabras que aparecen con una frecuencia superior.Se puede pasar un float entre 0 y 1 inclusive o un entero, si se pasa un valor de tipo float el límite se toma como la fecuencia de aparición de la palabra en documentos, si se pasa un valor entero el límite se toma como la cantidad absoluta de apariciones de la palabra en todos los documentos.\n",
    "En el gridsearch se probó con los valores 0.5 y 1.0, que significan respecivamente,eliminar las palabras que aparecen en la mitad o más de los documentos, y eliminar las palabras que aparecen en absoutamente todos los documentos.\n",
    "\n",
    "min_df : float in range [0.0, 1.0] or int, default=1\n",
    "Este parámetro es el análogo del anterior pero para el mínimo. No tiene en cuenta para el vocabulario las palabras que aparecen con frecuencia inferior a la marcada. Cumple la misma lógica de aceptar un valor de tipo float entre o y 1 o un entero positivo.\n",
    "Se probaron los valores 1 y 5, es decir que no se tengan en cuenta los términos que apaerecen menos de 1 y 5 veces respectivamente.\n",
    "\n",
    "max_features : int or None, default=None\n",
    "Si se utiliza un valor entero para este parámetro entonces el vocabulario considera solamente los primeros n términos con mayor frecuencia,siendo n el entero ingresado como max_features. Si se utiliza el valor None entonces se consideran todos los términos del vocabukario encontrado.\n",
    "Se usaron los valores 1000 y 2000 para probar.\n",
    "\n",
    "Estos tres parámetros son ignorados si countVectorizer toma un vocabulario inicial, ya que en ese caso el vocabulario ya esta definido y countVectorizer no arma uno propio,sino que considera solamente las palabras del vocabulario.\n",
    "\n",
    "norm : ‘l1’, ‘l2’ or None, optional\n",
    "Se elige que función se usa para normalizar los valores.En caso de usar el valor None no se hace normalización.\n",
    "En el gridsearch realizado se probó con las normalizaciones de tipo l1 y l2.\n",
    "\n",
    "use_idf : boolean, default=True\n",
    "Sirve para activar el balanceo por frecuencia inversa. Es decir que si se pasa el valor True(esto se hace por defecto) se usa la medida Tfidf , mientras que si se pasa el valor False se usa solo la medida Tf(Term frequency).\n",
    "Se realizó la prueba con el los valores True y False,es decir que se probó con las medidas TfidF y Tf respectivamente.\n",
    "\n",
    "sublinear_tf : boolean, default=False\n",
    "Define si se aplica ajuste sublineal a Tf.Si el valor es True se reemplaza tf con 1 + log(tf).\n",
    "Se realizó la prueba con los valores True y False.\n",
    "\n",
    "alpha : float, optional (default=1.0)\n",
    "Este parámetro como fue explicado en la pregunta sobre multinomialNB , se usa para el suavizado del cálculo de probabilidades empíricas. Sirve para que los términos que no aparecen en los casos de entrenamiento puedan tener una probabilidad por más que no se hayan encontrado ocurrencias en los documentos usados para entrenar.\n",
    "Para probar con gridSearch se usaron los valores 0.1 y 0.001\n",
    "\n",
    "Los parámetros que mejor resultado dieron se muestran en el resultado del cñodigo de la pregunta anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resultado promedio para KNeighbors Classifier\n",
      "0.927688606562\n",
      "\n",
      "Resultado promedio para MultinomialNB Classifier\n",
      "0.956093479643\n",
      "\n",
      "Resultado promedio para DecisionTree Classifier\n",
      "0.820443520668\n",
      "\n",
      "Resultado promedio para Multi-layer Perceptron Classifier\n",
      "0.980625563001\n",
      "\n",
      "Mejor clasificador: Multi-layer Perceptron Classifier Score: 0.980626 \n",
      "\n",
      "Evaluacion contra conjunto de pruebas\n",
      "Precision: 0.955958   Recall: 0.954946   F-score: 0.954942 \n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "names = ['KNeighbors Classifier','MultinomialNB Classifier','DecisionTree Classifier','Multi-layer Perceptron Classifier']\n",
    "models = list()\n",
    "\n",
    "pipeline_KN = Pipeline([('count_vect', CountVectorizer(max_df = best_parameters['count_vect__max_df'],max_features=best_parameters['count_vect__max_features'],min_df=best_parameters['count_vect__min_df']))\n",
    "                        ,('tfidf_transformer',TfidfTransformer(norm=best_parameters['tfidf_transformer__norm'],sublinear_tf=['tfidf_transformer__sublinear_tf'],use_idf=best_parameters['tfidf_transformer__use_idf'])),\n",
    "                        ('fs',SelectKBest(chi2, k=15000)),('clf',KNeighborsClassifier()),])\n",
    "models.append(pipeline_KN)\n",
    "pipeline_MNB = Pipeline([('count_vect', CountVectorizer(max_df = best_parameters['count_vect__max_df'],max_features=best_parameters['count_vect__max_features'],min_df=best_parameters['count_vect__min_df']))\n",
    "                        ,('tfidf_transformer',TfidfTransformer(norm=best_parameters['tfidf_transformer__norm'],sublinear_tf=['tfidf_transformer__sublinear_tf'],use_idf=best_parameters['tfidf_transformer__use_idf'])),\n",
    "                        ('fs',SelectKBest(chi2, k=15000)),('clf',MultinomialNB()),])\n",
    "models.append(pipeline_MNB)\n",
    "pipeline_DTree= Pipeline([('count_vect', CountVectorizer(max_df = best_parameters['count_vect__max_df'],max_features=best_parameters['count_vect__max_features'],min_df=best_parameters['count_vect__min_df']))\n",
    "                        ,('tfidf_transformer',TfidfTransformer(norm=best_parameters['tfidf_transformer__norm'],sublinear_tf=['tfidf_transformer__sublinear_tf'],use_idf=best_parameters['tfidf_transformer__use_idf'])),\n",
    "                        ('fs',SelectKBest(chi2, k=15000)),('clf',DecisionTreeClassifier()),])\n",
    "models.append(pipeline_DTree)\n",
    "pipeline_MLPC= Pipeline([('count_vect', CountVectorizer(max_df = best_parameters['count_vect__max_df'],max_features=best_parameters['count_vect__max_features'],min_df=best_parameters['count_vect__min_df']))\n",
    "                        ,('tfidf_transformer',TfidfTransformer(norm=best_parameters['tfidf_transformer__norm'],sublinear_tf=['tfidf_transformer__sublinear_tf'],use_idf=best_parameters['tfidf_transformer__use_idf'])),\n",
    "                        ('fs',SelectKBest(chi2, k=15000)),('clf',MLPClassifier()),])\n",
    "models.append(pipeline_MLPC)\n",
    "\n",
    "max_score = {'BestClassifier_name':'','BestClassifier_score':0,'BestClassifier_model':None}\n",
    "## Prueba mejor clasificador##\n",
    "for model,name in zip(models,names):   \n",
    "    print '\\nResultado promedio para %s' % (name)\n",
    "    score = np.mean(cross_val_score(model, train.data, train.target, cv=5, scoring='accuracy'))  \n",
    "    print score\n",
    "    if score > max_score['BestClassifier_score']:\n",
    "        max_score['BestClassifier_name'] = name\n",
    "        max_score['BestClassifier_score'] = score\n",
    "        max_score['BestClassifier_model'] = model\n",
    "        \n",
    "print '\\nMejor clasificador: %s Score: %f \\n' % (max_score['BestClassifier_name'],max_score['BestClassifier_score']) \n",
    "\n",
    "## Prueba contra conjunto de testing ##\n",
    "print \"Evaluacion contra conjunto de pruebas\"\n",
    "best_model = max_score['BestClassifier_model']\n",
    "best_model.fit(train.data,train.target)\n",
    "predicted = best_model.predict(test.data)\n",
    "res =  precision_recall_fscore_support(test.target, predicted, average='macro')\n",
    "print \"Precision: %f \" % res[0],\n",
    "print \" Recall: %f \" % res[1],\n",
    "print \" F-score: %f \" % res[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repita el ejercicio utilizando HashingVectorizer para extraer características. Explique las ventajas de este método de extracción de caraterísticas frente a CountVectorizer.\n",
    "\n",
    "Respuesta:\n",
    "\n",
    "Como se explico anteriormente, CountVectorizer considera los documentos como una bolsa de palabras, tomando en cuenta \n",
    "únicamente la cantidad de ocurrencias. Esta estrategia tiene dos problemas, el primero es que al considerar tokens como palabras individuales no estamos considerando frases y expresiones (formadas por varias palabras) en los documentos, esta información puede aportar para la clasificación si se logra extraerla. \n",
    "El segundo problema que presenta es la incapacidad de detectar faltas ortográficas, como ejemplo supongamos dos documentos con cuerpos \"arbol\" y \"abrol\", el segundo tiene una falta de ortografía, pero CountVectorizer las considera palabras distintas. Para solucionarlo una estrategia puede ser considerar tokens formados por caracteres de largo n (incluyendo los espacios), de esta forma el resultado de tokenizar nuestro ejemplo sería ['ar','rb','bo','ol','ab','br','ro'], teniendo las dos palabras en comun por lo menos algunas features. A este resultado se le llama \"n-grams collection\" mientras que la bolsa de palabras es una \"unigram collection\".\n",
    "La aplicacion descrita en el parrafo anterior es simple pero tiene algunos problemas sobre todo de performance y uso de memoria.\n",
    "La libreria HashingVectorizer los soluciona, la diferecnia entre CountVectorizer y HashingVectorizer es la forma en la que Hashingvectorizer ¨tokeniza¨ las palabras. CountVectorizer calcula la cantidad de ocurrencias para cada palabra. HashingVectorizer lo que hace es usar una función de Hashing para asociar de manera veloz un número a determinada palabra del documento y de esta forma aplica las etapas de ¨tokenizar¨ y contar en una misma etapa.\n",
    "\n",
    "En resumen, HashingVectorizer combina las tecnicas Hashing y CountVectorizer para mejorar los resultados.\n",
    "\n",
    "Ventajas de HashingVectorizer:\n",
    "-Del punto de vista de memoria es muy escalable para datasets muy grandes,ya que no tiene necesidad de almacenar un diccionario del vocabulario en memoria.\n",
    "-Es rápido para serialización y deserialización ya que no tiene una estado asociado.\n",
    "Puede ser usado en un streaming (partial fit) o parallel pipeline ya que no se computa ningún estado durante el fit.\n",
    "\n",
    "Desventajas:\n",
    "-Si bien podemos transformar un documento en un conjunto de tokens y luego en indices de las features, no es posible hacer el proceso inverso, esto puede dar problema a la hora de analizar los resultados por ejemplo para ver que features son mas relevantes para un grupo.\n",
    "-Puede haber colisiones y almacenarse dos tokens distintos dentro de un mismo indice de feature.\n",
    "-No aplica IDF weighting como lo haria un transformador con estado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resultado cross-validation con HashingVectorizer para el MultinomialNB\n",
      "[ 0.92060086  0.92903226  0.95268817  0.95913978  0.95238095]\n",
      "\n",
      "Mejores hiper-parametros encontraodos para el HashingVectorizer\n",
      "\thash_vect__lowercase: True\n",
      "\thash_vect__n_features: 262144\n",
      "\thash_vect__ngram_range: (1, 1)\n",
      "\thash_vect__norm: None\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "pipeline_MNB_ = Pipeline([('hash_vect', HashingVectorizer(non_negative=True)),('tfidf_transformer',TfidfTransformer()),('clf',MultinomialNB()),])\n",
    "\n",
    "print \"Resultado cross-validation con HashingVectorizer para el MultinomialNB\"\n",
    "print cross_val_score(pipeline_MNB_, train.data, train.target, cv=5, scoring='accuracy')\n",
    "\n",
    "tuned_parameters = {\n",
    "    'hash_vect__ngram_range': ((2,2),(1,1)),\n",
    "    'hash_vect__n_features':(2**18,2**19,2**20),\n",
    "    'hash_vect__lowercase':(True,False),\n",
    "    'hash_vect__norm':(None,'l1')  \n",
    "   }\n",
    "grid_search = GridSearchCV(pipeline_MNB_, tuned_parameters, cv=3,scoring='accuracy',n_jobs = 2)\n",
    "grid_search.fit(train.data, train.target)\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "print '\\nMejores hiper-parametros encontraodos para el HashingVectorizer'\n",
    "for param_name in sorted(list(tuned_parameters.keys())): print(\"\\t{0}: {1}\".format(param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
